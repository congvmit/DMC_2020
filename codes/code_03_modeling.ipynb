{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SETTINGS\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv('../data/prepared/df_v11.csv',      compression = 'gzip')\n",
    "df_test  = pd.read_csv('../data/prepared/df_test_v11.csv', compression = 'gzip')\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target\n",
    "y = df_train['target']\n",
    "X = df_train.drop('target', axis = 1)\n",
    "del df_train\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# format test data\n",
    "X_test = df_test.drop('target', axis = 1)\n",
    "del df_test\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read items\n",
    "items = pd.read_csv('../data/prepared/items_v1.csv', compression = 'gzip')\n",
    "print(items.shape)\n",
    "\n",
    "# keep existing IDs\n",
    "items = items[items['itemID'].isin(X['itemID'].unique())]\n",
    "print(items.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORT EVALUATION FUNCTIUONS\n",
    "\n",
    "# profit function\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import asymmetric_mse, profit, postprocess_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LIST RELEVANT FEATURES\n",
    "\n",
    "drop_feats = ['itemID', 'day_of_year'] + ['category1', 'category2', 'category3'] #+ list(X.filter(like = '_all_).columns\n",
    "features = [var for var in X.columns if var not in drop_feats]\n",
    "print(len(features), 'features')\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## MODELING PARAMETERS\n",
    "\n",
    "### DATA PARTITIONING\n",
    "\n",
    "# paritioning\n",
    "num_folds = 7   # no. CV folds\n",
    "test_days = 14  # no. days in the test set\n",
    "\n",
    "# settings\n",
    "seed  = 23\n",
    "\n",
    "\n",
    "### TRAINING OPTIONS\n",
    "\n",
    "# target transformation\n",
    "target_transform = True\n",
    "\n",
    "# train on positive sales only\n",
    "train_on_positive = False\n",
    "\n",
    "# two-stage model\n",
    "two_stage = True\n",
    "\n",
    "# use tuned meta-params\n",
    "tuned_params = True\n",
    "\n",
    "\n",
    "### CLASSIFIER PARAMETERS\n",
    "\n",
    "# rounds and options\n",
    "cores       = 4\n",
    "stop_rounds = 100\n",
    "verbose     = 100\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    'boosting_type':    'goss',\n",
    "    'objective':        'rmse',#asymmetric_mse,\n",
    "    'metrics':          'rmse',\n",
    "    'n_estimators':     1000,\n",
    "    'learning_rate':    0.1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1':        0.1,\n",
    "    'lambda_l2':        0.1,\n",
    "    'silent':           True,\n",
    "    'verbosity':        -1,\n",
    "    'nthread' :         cores,\n",
    "    'random_state':     seed,\n",
    "}\n",
    "\n",
    "# load optimal parameters\n",
    "if tuned_params:\n",
    "    par_file   = open('../lgb_meta_params_100.pkl', 'rb')\n",
    "    lgb_params = pickle.load(par_file)\n",
    "    lgb_params['nthread']      = cores\n",
    "    lgb_params['random_state'] = seed\n",
    "\n",
    "# second-stage LGB\n",
    "if two_stage:\n",
    "    lgb_classifier_params              = lgb_params.copy()\n",
    "    lgb_classifier_params['objective'] = 'binary'\n",
    "    lgb_classifier_params['metrics']   = 'logloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CROSS-VALIDATION LOOP\n",
    "\n",
    "# placeholders\n",
    "importances   = pd.DataFrame()\n",
    "preds_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "reals_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "preds_test    = np.zeros(items.shape[0])\n",
    "oof_rmse      = []\n",
    "oof_profit    = []\n",
    "oracle_profit = []\n",
    "clfs          = []\n",
    "train_idx     = []\n",
    "valid_idx     = []\n",
    "\n",
    "# objects\n",
    "train_days = X['day_of_year'].max() - test_days + 1 - num_folds - X['day_of_year'].min() # no. days in the train set\n",
    "time_start = time.time()\n",
    "\n",
    "# modeling loop\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### PARTITIONING\n",
    "    \n",
    "    # validation dates\n",
    "    if fold == 0:\n",
    "        v_end = X['day_of_year'].max()\n",
    "    else:\n",
    "        v_end = v_end - 1\n",
    "    v_start = v_end\n",
    "    \n",
    "    # training dates\n",
    "    t_end   = v_start - (test_days + 1)\n",
    "    t_start = t_end   - (train_days - 1)\n",
    "    \n",
    "    # extract index\n",
    "    train_idx.append(list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index))\n",
    "    valid_idx.append(list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index))   \n",
    "    \n",
    "    # extract samples\n",
    "    X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]]\n",
    "    X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]]\n",
    "    X_test = X_test[features]\n",
    "    \n",
    "    # keep positive cases\n",
    "    if train_on_positive:\n",
    "        y_train = y_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "        X_train = X_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "\n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx[fold])))\n",
    "    print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx[fold])))\n",
    "    print('-' * 65)\n",
    "\n",
    "    \n",
    "    ##### MODELING\n",
    "    \n",
    "    # target transformation\n",
    "    if target_transform:\n",
    "        y_train = np.sqrt(y_train)\n",
    "        y_valid = np.sqrt(y_valid)\n",
    "        \n",
    "    # first stage model\n",
    "    if two_stage:\n",
    "        y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy()\n",
    "        y_train_binary[y_train_binary > 0] = 1\n",
    "        y_valid_binary[y_valid_binary > 0] = 1\n",
    "        clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) \n",
    "        clf_classifier = clf_classifier.fit(X_train, y_train_binary, \n",
    "                                            eval_set              = [(X_train, y_train_binary), (X_valid, y_valid_binary)],\n",
    "                                            eval_metric           = 'logloss',\n",
    "                                            early_stopping_rounds = stop_rounds,\n",
    "                                            verbose               = verbose)\n",
    "        preds_oof_fold_binary  = clf_classifier.predict(X_valid)\n",
    "        preds_test_fold_binary = clf_classifier.predict(X_test)\n",
    "\n",
    "    # training\n",
    "    clf = lgb.LGBMRegressor(**lgb_params) \n",
    "    clf = clf.fit(X_train, y_train, \n",
    "                  eval_set              = [(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_metric           = 'rmse',\n",
    "                  sample_weight         = X_train['simulationPrice'].values,\n",
    "                  eval_sample_weight    = [X_train['simulationPrice'].values, X_valid['simulationPrice'].values],\n",
    "                  early_stopping_rounds = stop_rounds,\n",
    "                  verbose               = verbose)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # inference\n",
    "    if target_transform:      \n",
    "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid)**2)\n",
    "        reals_oof_fold  = y_valid**2\n",
    "        preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds\n",
    "    else:\n",
    "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid))\n",
    "        reals_oof_fold  = y_valid\n",
    "        preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds\n",
    "        \n",
    "    # impute zeros\n",
    "    if train_on_positive:\n",
    "        preds_oof_fold[(X_valid['order_sum_last_28'] == 0) & (X_valid['promo_in_test'] == 0)] = 0\n",
    "        preds_test_fold[(X_test['order_sum_last_28'] == 0) & (X_test['promo_in_test']  == 0)] = 0\n",
    "        \n",
    "    # multiply with first stage predictions\n",
    "    if two_stage:\n",
    "        preds_oof_fold  = preds_oof_fold  * np.round(preds_oof_fold_binary)\n",
    "        preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary)\n",
    "\n",
    "    # write predictions\n",
    "    preds_oof[fold, :] = preds_oof_fold\n",
    "    reals_oof[fold, :] = reals_oof_fold\n",
    "    preds_test        += preds_test_fold\n",
    "        \n",
    "        \n",
    "    ##### EVALUATION\n",
    "\n",
    "    # evaluation\n",
    "    oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], \n",
    "                                               preds_oof[fold, :])))\n",
    "    oof_profit.append(profit(reals_oof[fold, :], \n",
    "                             preds_oof[fold, :], \n",
    "                             price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    oracle_profit.append(profit(reals_oof[fold, :], \n",
    "                                reals_oof[fold, :], \n",
    "                                price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    \n",
    "    # feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df['Feature'] = features\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold'] = fold + 1\n",
    "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}'.format(fold + 1, \n",
    "                                                                  num_folds, \n",
    "                                                                  oof_rmse[fold], \n",
    "                                                                  oof_profit[fold]))\n",
    "    print('-' * 65)\n",
    "    print('')\n",
    "    \n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('-' * 65)\n",
    "print('- AVERAGE RMSE:   {:.2f}'.format(np.mean(oof_rmse)))\n",
    "print('- AVERAGE PROFIT: {:.0f} ({:.2f}%)'.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit)))\n",
    "print('- RUNNING TIME:   {:.2f} minutes'.format((time.time() - time_start) / 60))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATION\n",
    "\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "# residual plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1))\n",
    "axis_lim = np.max([reals_oof.max(), preds_oof.max()])\n",
    "plt.ylim(top   = 1.02*axis_lim)\n",
    "plt.xlim(right = 1.02*axis_lim)\n",
    "plt.plot((0, axis_lim), (0, axis_lim), 'r--')\n",
    "plt.title('Residual Plot')\n",
    "plt.ylabel('Predicted demand')\n",
    "plt.xlabel('Actual demand')\n",
    "\n",
    "# feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "top_feats = 50\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False), ci = 0)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# export\n",
    "plt.savefig('../lgb_performance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOGS\n",
    "\n",
    "# model (RMSE, profit): description\n",
    "#\n",
    "# lgb_v1_df_v1  (102.49, -65.72%): lgb 17 features\n",
    "# lgb_v2_df_v2  (78.08, 20.69%):   lgb 18 features\n",
    "# lgb_v3_df_v3  (73.24, 20.22%):   lgb 18 features\n",
    "# lgb_v4_df_v3  (73.37, 21.09%):   lgb 18 features, train_days = 101\n",
    "# lgb_v5_df_v3  (72.82, 28.90%):   lgb 18 features, use asymmetric_mse as loss\n",
    "# lgb_v6_df_v3  (72.75, 35.30%):   lgb 18 features, use simulationPrice as training weights\n",
    "# lgb_v7_df_v3  (72.75, 35.66%):   lgb 18 features, use simulationPrice as train & eval weights\n",
    "# lgb_v7_df_v4  (72.31, 36.61%):   lgb 23 features\n",
    "# lgb_v8_df_v4  (75.33, 37.06%):   lgb 23 features, target transofrmation (squared root)\n",
    "# lgb_v8_df_v5  (76.08, 36.18%):   lgb 31 features, new data set (additional features)\n",
    "# lgb_v9_df_v5  (80.46, 30.95%):   lgb 23 features, train on positive cases only\n",
    "# lgb_v9_df_v6  (58.72, 51.52%):   lgb 23 features, new data set (promo tweak)\n",
    "# lgb_v9_df_v7  (63.49, 51.19%):   lgb 35 features, new data set (features on all orders/promos)\n",
    "# lgb_v9_df_v8  (63.00, 52.05%):   lgb 35 features, new data set (features on manufacturer orders/promos)\n",
    "# lgb_v10_df_v8 (62.03, 52.04%):   lgb 35 features, new data set (features on manufacturer orders/promos) RMSE loss\n",
    "# lgb_v11_df_v8 (61.93, 52.95%):   lgb 35 features, new data set (features on manufacturer orders/promos) RMSE loss goss boosting\n",
    "# lgb_v12_df_v8 (61.93, 52.95%):   lgb 35 features, new data set (features on manufacturer orders/promos)\n",
    "#                                  RMSE loss goss boosting, 100 early stopping\n",
    "# lgb_v12_df_v9  (61.51, 53.53%):  lgb 39 features, new data set with mean price ratios\n",
    "# lgb_v12_df_v10 (61.62, 53.58%):  lgb 49 features, new data set with 1-day lag features\n",
    "\n",
    "# lgb_v13_df_v11 (71.12, 52.47%):  lgb 62 features, new data set with fixed partitioning\n",
    "# lgb_v14_df_v11 (70.69, 52.89%):  lgb 74 features, onclude back lagged features per all items (_all_)\n",
    "# lgb_v15_df_v11 (70.69, 52.92%):  lgb 74 features, two-stage modeling (classifier => regressor)\n",
    "# lgb_v16_df_v11 (69.80, 53.57%):  lgb 74 features, optimal params from hyperopt (100 trials)\n",
    "# lgb_v17_df_v11 (69.80, 53.61%):  lgb 74 features, optimal params for both classifier and regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SUBMISSION\n",
    "\n",
    "# model name\n",
    "name = 'lgb_v17_df_v11'\n",
    "sub_name = name + '_profit_' + str(int(np.round(np.mean(oof_profit))))\n",
    "\n",
    "# save OOF preds\n",
    "oof = np.stack((preds_oof, reals_oof))\n",
    "np.save('../oof_preds/' + sub_name + '.npy', oof)\n",
    "print(oof.shape)\n",
    "\n",
    "# save submissiion\n",
    "sub = pd.read_csv('../submissions/sample_submission.csv', sep = '|')\n",
    "sub['demandPrediction'] = postprocess_preds(preds_test)\n",
    "sub.to_csv('../submissions/sub_' + sub_name + '.csv', sep = '|', index = False)\n",
    "print(sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### COMPARE WITH THE BEST SUBMISSION\n",
    "\n",
    "# import old submission\n",
    "best_name   = 'sub_lgb_v16_df_v11_profit_3939086.csv'\n",
    "best        = pd.read_csv('../submissions/' + str(best_name), sep = '|')\n",
    "\n",
    "# check correlation\n",
    "best['sub'] = sub['demandPrediction']\n",
    "print('- prediction orrelation: ' + str(best.corr()['sub']['demandPrediction']))\n",
    "print('- old prediction mean:   ' + str(best['demandPrediction'].mean()))\n",
    "print('- new prediction mean:   ' + str(best['sub'].mean()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
