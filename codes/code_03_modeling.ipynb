{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LIBRARIES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## SETTINGS\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('dark_background')\n",
    "%matplotlib inline\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv('../data/prepared/df_v5.csv',      compression = 'gzip')\n",
    "df_test  = pd.read_csv('../data/prepared/df_test_v5.csv', compression = 'gzip')\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract target\n",
    "y = df_train['target']\n",
    "X = df_train.drop('target', axis = 1)\n",
    "del df_train\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# format test data\n",
    "X_test = df_test.drop('target', axis = 1)\n",
    "del df_test\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read items\n",
    "items = pd.read_csv('../data/prepared/items_v1.csv', compression = 'gzip')\n",
    "print(items.shape)\n",
    "\n",
    "# keep existing IDs\n",
    "items = items[items['itemID'].isin(X['itemID'].unique())]\n",
    "print(items.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### IMPORT EVALUATION FUNCTIUONS\n",
    "\n",
    "# profit function\n",
    "import functions\n",
    "importlib.reload(functions)\n",
    "from functions import asymmetric_mse, profit, postprocess_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LIST RELEVANT FEATURES\n",
    "\n",
    "drop_feats = ['itemID', 'day_of_year',\n",
    "              'all_orders_count_last_7', 'all_orders_count_last_14', 'all_orders_count_last_21', 'all_orders_count_last_28',\n",
    "              'all_promos_count_last_7', 'all_promos_count_last_14', 'all_promos_count_last_21', 'all_promos_count_last_28',\n",
    "             ]\n",
    "features = [var for var in X.columns if var not in drop_feats]\n",
    "print(len(features), ':', features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MODELING PARAMETERS\n",
    "\n",
    "# paritioning\n",
    "num_folds = 7   # no. CV folds\n",
    "test_days = 14  # no. days in the test set\n",
    "\n",
    "# settings\n",
    "cores = 4\n",
    "seed  = 23\n",
    "\n",
    "# rounds abd verbose\n",
    "stop_rounds = 10\n",
    "verbose     = 20\n",
    "\n",
    "# target transformation\n",
    "target_transform = True\n",
    "\n",
    "# train on positive sales only\n",
    "train_on_positive = False\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    'boosting_type':    'gbdt',\n",
    "    'objective':        asymmetric_mse,\n",
    "    'metrics':          'rmse',\n",
    "    'learning_rate':    0.1,\n",
    "    'n_estimators':     1000,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1':        0.1,\n",
    "    'lambda_l2':        0.1,\n",
    "    'silent':           True,\n",
    "    'verbosity':        -1,\n",
    "    'nthread' :         cores,\n",
    "    'random_state':     seed,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CROSS-VALIDATION LOOP\n",
    "\n",
    "# placeholders\n",
    "importances   = pd.DataFrame()\n",
    "preds_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "reals_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "preds_test    = np.zeros(items.shape[0])\n",
    "oof_rmse      = []\n",
    "oof_profit    = []\n",
    "oracle_profit = []\n",
    "clfs          = []\n",
    "train_idx     = []\n",
    "valid_idx     = []\n",
    "\n",
    "# objects\n",
    "train_days = X['day_of_year'].max() - 2*test_days - num_folds - X['day_of_year'].min()  # no. days in the train set\n",
    "time_start = time.time()\n",
    "\n",
    "# modeling loop\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### PARTITIONING\n",
    "    \n",
    "    # validation dates\n",
    "    if fold == 0:\n",
    "        v_end = X['day_of_year'].max() - (test_days + 1)\n",
    "    else:\n",
    "        v_end = v_end - 1\n",
    "    v_start = v_end\n",
    "    \n",
    "    # training dates\n",
    "    t_end   = v_start - (test_days + 1)\n",
    "    t_start = t_end   - (train_days - 1)\n",
    "    \n",
    "    # extract index\n",
    "    train_idx.append(list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index))\n",
    "    valid_idx.append(list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index))   \n",
    "    \n",
    "    # extract samples\n",
    "    X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]]\n",
    "    X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]]\n",
    "    X_test = X_test[features]\n",
    "    \n",
    "    # keep positive cases\n",
    "    if train_on_positive:\n",
    "        y_train = y_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "        X_train = X_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "\n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx[fold])))\n",
    "    print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx[fold])))\n",
    "    print('-' * 65)\n",
    "\n",
    "    \n",
    "    ##### MODELING\n",
    "    \n",
    "    # target transformation\n",
    "    if target_transform:\n",
    "        y_train = np.sqrt(y_train)\n",
    "        y_valid = np.sqrt(y_valid)\n",
    "\n",
    "    # training\n",
    "    clf = lgb.LGBMRegressor(**lgb_params) \n",
    "    clf = clf.fit(X_train, y_train, \n",
    "                  eval_set              = [(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_metric           = 'rmse',\n",
    "                  sample_weight         = X_train['simulationPrice'].values,\n",
    "                  eval_sample_weight    = [X_train['simulationPrice'].values, X_valid['simulationPrice'].values],\n",
    "                  early_stopping_rounds = stop_rounds,\n",
    "                  verbose               = verbose)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # inference\n",
    "    if target_transform:      \n",
    "        preds_oof[fold, :] = postprocess_preds(clf.predict(X_valid)**2)\n",
    "        reals_oof[fold, :] = y_valid**2\n",
    "        preds_test += postprocess_preds(clf.predict(X_test)**2) / num_folds\n",
    "    else:\n",
    "        preds_oof[fold, :] = postprocess_preds(clf.predict(X_valid))\n",
    "        reals_oof[fold, :] = y_valid\n",
    "        preds_test += postprocess_preds(clf.predict(X_test)) / num_folds\n",
    "        \n",
    "    # impute zeros\n",
    "    if train_on_positive:\n",
    "        preds_oof[fold, :][(X_valid['order_sum_last_28'] == 0) & (X_valid['promo_in_test'] == 0)] = 0\n",
    "        preds_test[(X_test['order_sum_last_28'] == 0) & (X_test['promo_in_test'] == 0)] = 0\n",
    "        \n",
    "        \n",
    "    ##### EVALUATION\n",
    "\n",
    "    # evaluation\n",
    "    oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], \n",
    "                                               preds_oof[fold, :])))\n",
    "    oof_profit.append(profit(reals_oof[fold, :], \n",
    "                             preds_oof[fold, :], \n",
    "                             price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    oracle_profit.append(profit(reals_oof[fold, :], \n",
    "                                reals_oof[fold, :], \n",
    "                                price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    \n",
    "    # feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df['Feature'] = features\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold'] = fold + 1\n",
    "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}'.format(fold + 1, \n",
    "                                                                  num_folds, \n",
    "                                                                  oof_rmse[fold], \n",
    "                                                                  oof_profit[fold]))\n",
    "    print('-' * 65)\n",
    "    print('')\n",
    "    \n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('-' * 65)\n",
    "print('- AVERAGE RMSE: {:.2f}'.format(np.mean(oof_rmse)))\n",
    "print('- TOTAL PROFIT: {:.0f} ({:.2f}%)'.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit)))\n",
    "print('- RUNNING TIME: {:.2f} minutes'.format((time.time() - time_start) / 60))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATION\n",
    "\n",
    "fig = plt.figure(figsize = (12, 5))\n",
    "\n",
    "# residual plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1))\n",
    "axis_lim = np.max([reals_oof.max(), preds_oof.max()])\n",
    "plt.ylim(top   = 1.02*axis_lim)\n",
    "plt.xlim(right = 1.02*axis_lim)\n",
    "plt.plot((0, axis_lim), (0, axis_lim), 'r--')\n",
    "plt.title('Residual Plot')\n",
    "plt.ylabel('Predicted demand')\n",
    "plt.xlabel('Actual demand')\n",
    "\n",
    "# feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "top_feats = 100\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False), ci = 0)\n",
    "plt.title('Feature Importance')\n",
    "plt.tight_layout()\n",
    "\n",
    "# export\n",
    "plt.savefig('../model_performance.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUBMISSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### LOGS\n",
    "\n",
    "# model (RMSE, profit): description\n",
    "#\n",
    "# lgb_v1_df_v1 (102.49, -65.72%): lgb 17 features\n",
    "# lgb_v2_df_v2 (78.08, 20.69%):   lgb 18 features\n",
    "# lgb_v3_df_v3 (73.24, 20.22%):   lgb 18 features\n",
    "# lgb_v4_df_v3 (73.37, 21.09%):   lgb 18 features, train_days = 101\n",
    "# lgb_v5_df_v3 (72.82, 28.90%):   lgb 18 features, use asymmetric_mse as loss\n",
    "# lgb_v6_df_v3 (72.75, 35.30%):   lgb 18 features, use simulationPrice as training weights\n",
    "# lgb_v7_df_v3 (72.75, 35.66%):   lgb 18 features, use simulationPrice as train & eval weights\n",
    "# lgb_v7_df_v4 (72.31, 36.61%):   lgb 23 features\n",
    "# lgb_v8_df_v4 (75.33, 37.06%):   lgb 23 features, target transofrmation (squared root)\n",
    "# lgb_v8_df_v5 (76.08, 36.18%):   lgb 31 features\n",
    "# lgb_v9_df_v5 (80.46, 30.95%):   lgb 23 features, train on positive cases only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SUBMISSION\n",
    "\n",
    "# model name\n",
    "name = 'lgb_v8_df_v4'\n",
    "sub_name = name + '_profit_' + str(int(np.round(np.mean(oof_profit))))\n",
    "\n",
    "# save OOF preds\n",
    "oof = np.stack((preds_oof, reals_oof))\n",
    "np.save('../oof_preds/' + sub_name + '.npy', oof)\n",
    "print(oof.shape)\n",
    "\n",
    "# save submissiion\n",
    "sub = pd.read_csv('../submissions/sample_submission.csv', sep = '|')\n",
    "sub['demandPrediction'] = postprocess_preds(preds_test)\n",
    "sub.to_csv('../submissions/sub_' + sub_name + '.csv', sep = '|', index = False)\n",
    "print(sub.shape)\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
